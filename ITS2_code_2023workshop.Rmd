---
title: "Symbiodiniaceae ITS2 analysis"
output: html_document
---

# This pipeline has been adapted from a published process that aimed tp complete ITS2 sequencing data and analysing the Symbiodiniaceae community composition in corals. The pipeline used is available on: https://github.com/LaserKate/MontiSymTransgen/blob/master/DADA2analysis_DESeq.R

# Before starting R analysis, the following commands run in the Terminal:

# 1) Download your files. Often, sequencing facility will provide a code (example below) to download from their storage location.
#wget -r -e robots=off -A gz,csv,html,txt,php -t 7 -w 5 --waitretry=14 https://files.cgrb.oregonstate.edu:443/Illumina/210709_M01498_0810_000000000-JR2K7/

# 2) Set working directory. Unzip the files, type:
# cd /Users/laurenhowe-kerr/Desktop/DADA2-ITS2-workshop/dada2-test-seqs/
# gunzip *.gz

# 3) Create new file (samples.list) containing the first column of R1 file names (ex. 2017-A01_S1_L001_R1_001.fastq to 2017-A01)
# ls *R1.fastq | cut -d '_' -f 1 > samples.list

# rename files (some of the files are names differently- selecting *R1* says ignore whats immediately before or after the R1, but grab any file that has it):
# for file in $(cat samples.list); do  mv ${file}_*R1*.fastq ${file}_R1.fastq; mv ${file}_*R2*.fastq ${file}_R2.fastq; done 

#Count number of lines (sequences) in all files (helpful for checking throughout process)
#echo $(cat *R[12].fastq|wc -l)/4|bc
# 712512

# 4) Some pre-trimming

#Remove reads containing Illumina sequencing adapters:
#Trim adapters; use bbduk - (may need to alter k and mink values for other markers)
# for file in $(cat samples.list); do /Users/laurenhowe-kerr/Documents/programs/bbmap/bbduk.sh in1=${file}_R1.fastq in2=${file}_R2.fastq ref=adaptors.fasta k=12 hdist=1 tpe tbo out1=${file}_R1_NoAdapt.fastq out2=${file}_R2_NoAdapt.fastq; done &>bbduk_NoAdapt.log

#check log file to make sure most reads were kept

#Retain only PE reads that match amplicon primer. For these samples, SYM_VAR_5.8SII (Hume et al. 2015) and SYM_VAR_REV  5’ (Hume et al. 2013) were used. restrictleft set based on primer length.
#for file in $(cat samples.list); do /Users/laurenhowe-kerr/Documents/programs/bbmap/bbduk.sh in1=${file}_R1_NoAdapt.fastq in2=${file}_R2_NoAdapt.fastq restrictleft=21 k=10 literal=GTGAATTGCAGAACTCCGTG,CCTCCGCTTACTTATATGCTT outm1=${file}_R1_NoIll_NoITS.fastq outu1=${file}_R1_check.fastq outm2=${file}_R2_NoIll_NoITS.fastq outu2=${file}_R2_check.fastq; done &>bbduk_NoITS.log

#gut check- most of the reads are still there, right?!
# echo $(cat *_NoITS.fastq|wc -l)/4|bc
# 612084 yes!

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Install packages
```{r}
if (!require("BiocManager", quietly = TRUE))
  install.packages("BiocManager")
BiocManager::install("dada2")
install.packages("phangorn")
BiocManager::install("Biostrings")
install.packages("optparse")
install.packages("tidyverse")
install.packages("ggplot2")
install.packages("scales")

```

## Call libraries

```{r calling libraries}


library(dada2); packageVersion("dada2"); citation("dada2") #processing of sequences
#library(ShortRead); packageVersion("ShortRead")
library(ggplot2); packageVersion("ggplot2") #data visualization
#library(phangorn)
#library(optparse)
#library(ggpubr)
#library(svglite)
library(tidyverse)


```


# ################################## Start of DADA analysis ##################################
#Dada2 assumes 3 things about samples
  #Samples have been demultiplexed, i.e. split into individual per-sample fastq files
  #Non-biological nucleotides have been removed, e.g. primers, adapters, etc
  #If paired-end sequencing data, the forward and reverse fastq files contain reads in matched order

```{r defining paths, echo=FALSE}

# Set path to trimmed fastq files
# Folder where unzipped, trimmed and renamed data are stored:
# All datasets are stored in the path variable folder and need to be unzipped 

path <- "/Users/laurenhowe-kerr/Desktop/DADA2-ITS2-workshop/dada2-test-seqs/filtered/"
fns <- list.files(path)
fns

```


##### Trimming/Filtering #######

```{r sorting reads, echo=FALSE}

fastqs <- fns[grepl(".fastq$", fns)]
fastqs <- sort(fastqs) # Sort ensures forward/reverse reads are in same order
fnFs <- fastqs[grepl(".R1.", fastqs)] # Just the forward read files
fnRs <- fastqs[grepl(".R2.", fastqs)] # Just the reverse read files

```


##### Get sample names, assuming files named as so: SAMPLENAME_XXX.fastq; OTHERWISE MODIFY #######

```{r specifying paths, echo=FALSE}
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1) #Get sample names, pulling out the first part of the sample file -- important for downstream analyses, need to be able to match to your metadata

# Specify the full path to the fnFs and fnRs
fnFs <- file.path(path, fnFs)
fnRs <- file.path(path, fnRs)

```

#### Visualize Raw data #######

```{r quality plots, echo=FALSE}

#First, lets look at quality profile of R1 reads
 
plotQualityProfile(fnFs[c(1,2,3,4)]) 

# The quality for forward reads drops at about 280 bp

#Then look at quality profile of R2 reads

#quartz()
plotQualityProfile(fnRs[c(1,2,3,4)])


# Quality above 30 is very good (kind of an arbitrary cutoff)
# Do not need to check every file- usually they should be similar

# "DADA2 incorporates quality information into its error model which makes the algorithm more robust,
# but trimming as the average qualities crash is still a good idea as long as our reads will still overlap. 

# "The distribution of quality scores at each position is shown as a grey-scale heat map,
# with dark colors corresponding to higher frequency. Green is the mean, orange is the median,
# and the dashed orange lines are the 25th and 75th quantiles."

# Recommend trimming where quality profile crashes - in this case, the quality for forward reads drops at about 260 bp (quality worse for reverse reads);for reverse around 220-230 bases it gets below 30; this leaves enough overlap

# "If using this workflow on your own data: Your reads must still overlap after truncation in order to merge
# them later! If you are using a less-overlapping primer set, your truncLen must be large enough to maintain
# 20 + biological.length.variation nucleotides of overlap between them.

# Not too many indels in ITS2; at least not super long ones (Indel refers to an insertion or deletion of bases in the genome of # an organism, can alter length- sometimes high variation in amplicon sizes for ITS and trimming to a fixed length will not be  #ideal)

```


#### Filtering ####


```{r filtering, echo=FALSE}
# Make directory and filenames for the filtered fastqs, just makes a path to rename files after the filtering step

filt_path <- file.path(path, "trimmed")
if(!file_test("-d", filt_path)) dir.create(filt_path)
filtFs <- file.path(filt_path, paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(filt_path, paste0(sample.names, "_R_filt.fastq.gz"))

# Filter
# specify where your F and R reads and where you want to put them after filtering
# specify where you want to trim your F and R reads, again we can aggressively trim here and there will still be overlap
# DADA does not allow Ns (ambiguous bases)
# max amount of estimated errors, this function did better at approximating the quality of the read than just the average quality score where EE = sum(10^(-Q/10)) (2 is fine baseline, here we allow 1 expected errors- we have high quality reads, lots of read depth so we can be stringent)
# a Q score of 2 is bad (~63% chance of a base call being incorrect) - this gets rid of your worst quality reads early
# N nucleotides to remove from the start of each read: ITS2 primers = F 20bp; R 21bp
# remove reads matching phiX genome (often added during sequencing to add diversity- helps illumina machine read your sequences correctly, most will be removed during demultiplexing)
# enforce matching between id-line sequence identifiers of F and R reads
# compress reduces memory needed by gzipping files
# On Windows set multithread=FALSE (TRUE allows multiple FASTQ files to be processed in parallel, speeds up processing in time, does not work on Windows)


out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, 
              truncLen=c(210,160), 
              maxN=0, 
              maxEE=c(1,1), 
              truncQ=2, 
              trimLeft=c(20,21),  
              rm.phix=TRUE, 
              matchIDs=TRUE, 
              compress=TRUE, multithread=TRUE) # On Windows set multithread=FALSE

head(out) 
tail(out)

```


#### Learn Error Rates #######

```{r error rates, echo=FALSE}

# "DADA2 learns its error model from the data itself by alternating estimation of the error rates and
# the composition of the sample until they converge on a jointly consistent solution
# As in many optimization problems, the algorithm must begin with an initial guess, for which the maximum
# possible error rates in this data are used (the error rates if only the most abundant sequence
# is correct and all the rest are errors)."

# setDadaOpt(MAX_CONSIST=30) #increase number of cycles to allow convergence if necessary
errF <- learnErrors(filtFs, multithread=TRUE)

errR <- learnErrors(filtRs, multithread=TRUE)
#has to do more rounds for R sequences, since they have more errors

# "sanity check: visualize estimated error rates
# error rates should decline with increasing qual score
# red line is based on definition of quality score alone
# black line is estimated error rate after convergence
# dots are observed error rate for each quality score"

plotErrors(errF, nominalQ=TRUE) 
plotErrors(errR, nominalQ=TRUE) 


```

##### Dereplicate reads #######

```{r dereplicate reads, echo=FALSE}

# "Dereplication combines all identical sequencing reads into into “unique sequences” with a corresponding
# “abundance”: the number of reads with that unique sequence. 
# Dereplication substantially reduces computation time by eliminating redundant comparisons.
# DADA2 retains a summary of the quality information associated with each unique sequence. The consensus quality
# profile of a unique sequence is the average of the positional qualities from the dereplicated reads.
# These quality profiles inform the error model of the subsequent denoising step,
# significantly increasing DADA2’s accuracy."

# Some samples were discarded during the trimming and filtering, so we should only select the existing ones:

exists <- file.exists(filtFs)
derepFs <- derepFastq(filtFs[exists], verbose=TRUE)
derepRs <- derepFastq(filtRs[exists], verbose=TRUE)

names(derepFs) <- sample.names[exists]
names(derepRs) <- sample.names[exists]

```

##### Infer Sequence Variants #######

```{r infere sequence variants, echo=FALSE}
# main part of DADA2!
# Uses error model developed earlier, calculates abundance p values for each unique sequence
# tests null hypothesis that a sequence with given error rate is too abundant to be explained by sequencing errors

# "Must change some of the DADA options b/c original program optimized for 16S/18S,
# note ITS - from github, "We currently recommend BAND_SIZE=32 for ITS data."


setDadaOpt(BAND_SIZE=32)

dadaFs <- dada(derepFs, err=errF, multithread=TRUE)
dadaRs <- dada(derepRs, err=errR, multithread=TRUE)

# "now, look at the dada class objects by sample
# will tell how many 'real' variants in unique input seqs
# By default, the dada function processes each sample independently, but pooled processing is available
# with pool=TRUE and that may give better results for low sampling depths at the cost of increased
# computation time. See our discussion about pooling samples for sample inference." 

dadaFs[[3]]
dadaRs[[3]]

```


##### Merge paired reads #######

```{r merge paired end reads, echo=FALSE}

# "To further cull spurious sequence variants
# Merge the denoised forward and reverse reads
# Paired reads that do not exactly overlap are removed"

mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)

# Inspect the merger data.frame from the first sample
head(mergers[[1]])

summary((mergers[[1]]))

# "We now have a data.frame for each sample with the merged $sequence, its $abundance,
# and the indices of the merged $forward and $reverse denoised sequences. Paired reads that did not
# exactly overlap were removed by mergePairs.


```


##### Construct sequence table #######

```{r construct sequence table, echo=FALSE}

# "A higher-resolution version of the “OTU table” produced by classical methods

seqtab <- makeSequenceTable(mergers)
dim(seqtab) # 10 x 421

# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))

plot(table(nchar(getSequences(seqtab)))) #real variants appear to be right in that 294-304 window

# "The sequence table is a matrix with rows corresponding to (and named by) the samples, and 
# columns corresponding to (and named by) the sequence variants. 
# Do merged sequences all fall in the expected range for amplicons? 
#The target amplicon was approximately 234-266 base pairs long, accept 229-271- nothing shorter than 241 tho 
# Sequences that are much longer or shorter than expected may be the result of non-specific priming,
# and may be worth removing

seqtab2 <- seqtab[,nchar(colnames(seqtab)) %in% seq(241, 271)] #again, being fairly conservative with length

table(nchar(getSequences(seqtab2)))
dim(seqtab2) # 10 x 363

```


##### Remove chimeras #######

```{r remove chimeras, echo = FALSE}
# can get these amplicons that are actually fusions of two parent sequences- from sequencing errors-- dada2 will recognize this and remove from your sequence table

# "The core dada method removes substitution and indel errors, but chimeras remain. 
# Fortunately, the accuracy of the sequences after denoising makes identifying chimeras easier 
# than it is when dealing with fuzzy OTUs: all sequences which can be exactly reconstructed as 
# a bimera (two-parent chimera) from more abundant sequences".

seqtab.nochim <- removeBimeraDenovo(seqtab2, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim) # 10 x 132: Identified 231 bimeras out of 363 input sequences.

sum(seqtab.nochim)/sum(seqtab2) # 0.9524372

# "The fraction of chimeras varies based on factors including experimental procedures and sample complexity, 
# but can be substantial.
# BUT those variants account for only a minority of the total sequence reads
# Most of your reads should remain after chimera removal (it is not uncommon for a majority of sequence
# variants to be removed though)"

```

##### Track Read Stats #######

```{r track read stats, echo = FALSE}

getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(mergers, getN), rowSums(seqtab2), rowSums(seqtab.nochim))
colnames(track) <- c("input", "filtered", "denoised", "merged", "tabled", "nonchim")
rownames(track) <- sample.names
head(track)
tail(track)

path <- "/Users/laurenhowe-kerr/Desktop/DADA2-ITS2-workshop"
write.csv(track,file="ReadFilterStats_testData.csv",row.names=TRUE,quote=FALSE)

# Now, save outputs so can come back to the analysis stage at a later point if desired
saveRDS(seqtab.nochim, file="SubSam_seqtab_nochim.rds")

```


##### Prepparing to Assign Taxonomy #######

```{r assign taxonomy, echo = FALSE}

# output 'Sequence counts' table 

#seqtab.nochim is the 'ASV counts' table...but is a little unwieldy
#want fasta file of 'ASVs' and table designated by 'ASV'

#First, output fasta file for 'ASVs'
path='/Users/laurenhowe-kerr/Desktop/DADA2-ITS2-workshop/seqtab.nochim-test.fasta'
uniquesToFasta(seqtab.nochim, path, ids = NULL, mode = "w", width = 20000)

seqtab.nochim_full <- seqtab.nochim
#then, rename output table and write it out
ids <- paste0("sq", seq(1, length(colnames(seqtab.nochim))))
colnames(seqtab.nochim)<-ids

write.csv(seqtab.nochim,file="OutputDADA_AllASVs.csv",quote=F) #this is your easily read-able ASV counts table!

str(seqtab.nochim)

# Stopping here
```


